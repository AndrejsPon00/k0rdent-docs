--- a/kof-retention.md
 b/kof-retention.md
@@ -0,0 1,38 @@

# KOF Retention and Replication

KOF stores metrics in **VictoriaMetrics** and logs in **VictoriaLogs**. Configure retention and replication to balance cost, durability, and compliance.

For example, consider these recommendations:

* **Management cluster:** short-term retention (1â€“30 days)
* **Regional clusters:** long-term retention (30â€“365 days)
* Increase **replicationFactor** where higher availability is required; this field enables you to determine how many copies are stored, usually on different nodes.

Configure VictoriaMetrics and VictoriaLogs by adjusting the `charts/kof-storage/values.yaml` file
to include the following parameters:

```yaml
victoriametrics:
  vmcluster:
    spec:
      retentionPeriod: "30d"
      replicationFactor: 2
      vmstorage:
        storage:
          volumeClaimTemplate:
            spec:
              resources:
                requests:
                  storage: 100Gi

victoria-logs-cluster:
  vlstorage:
    extraArgs:
      retentionPeriod: "30d"
    persistentVolume:
      size: "100Gi"
```
--- a/kof-tracing.md
 b/kof-tracing.md
@@ -0,0 1,12 @@

# KOF Tracing

KOF integrates **Jaeger** for distributed tracing via OpenTelemetry.. By default, Jaeger runs with in-memory storage and a maximum of **100,000 traces**. When the limit is reached, it evicts the oldest traces (FIFO). Note that in-memory traces are lost on pod restart.

 To solve this problem in production, you canuse a persistent backend (e.g., Cassandra, Elasticsearch, or a compatible VictoriaMetrics-Jaeger deployment) and set retention according to your requirements.

 For example, you can tell Jaeger to use Cassandra by adding the following to the `charts/kof-storage/values.yaml` file:

```yaml
jaeger:
  enabled: true
  spec:
    strategy: production
    storage:
      type: cassandra
      options:
        cassandra:
          servers: cassandra.kof.svc
          keyspace: jaeger_v1_dc1
          replication: "{'class':'NetworkTopologyStrategy','dc1':3}"
```

--- a/kof-faq.md
 b/kof-faq.md
@@ -0,0 1,18 @@

# KOF FAQ & Scenarios

## What is fullâ€‘stack observability in KOF?
OpenTelemetry collects **metrics**, **logs**, and **traces**; data is stored in **VictoriaMetrics**, **VictoriaLogs**, and **Jaeger**, and visualized in **Grafana**.

## How do I collect telemetry from a new service?

There are two ways to collect telemetry from a new service:

* Prometheus scrape annotations: Add scrape annotations to your pods so metrics are collected automatically.
* Auto-instrumentation: Let KOF inject the OpenTelemetry language agent into your pods at runtime.

For example, let's say you were adding auto-instrumentation to a Java Spring Boot service.  You'd follow these steps:

1. Enable instrumentation in **your application's** Helm chart values:
   ```yaml
   kof:
     instrumentation:
       enabled: true
       language: java
   ```
2. The Helm flag `kof.instrumentation.enabled=true` makes sure the necessary CRDs from the OpenTelemetry Operator are present in the cluster, but you still need to configure the actual application. When you deploy, add the appropriate annotation:
   ```yaml
   apiVersion: apps/v1
   kind: Deployment
   metadata:
     name: my-java-service
     namespace: demo
   spec:
     replicas: 1
     selector:
       matchLabels:
         app: my-java-service
     template:
       metadata:
         labels:
           app: my-java-service
         annotations:
           instrumentation.opentelemetry.io/inject-java: "true"
       spec:
         containers:
         - name: app
           image: myregistry/my-java-service:1.0.0
           ports:
           - containerPort: 8080
   ```

   With this annotation, KOF automatically injects the OpenTelemetry Java agent. The service then exports metrics, traces, and logs through the configured OpenTelemetry Collector.

3. After deploying, you should see:
   - Traces in Jaeger
   - Metrics in VictoriaMetrics (via Grafana dashboards)  
   - Logs in VictoriaLogs

For other languages, use the appropriate annotation, as in:

- Python: `instrumentation.opentelemetry.io/inject-python: "true"`
- Node.js: `instrumentation.opentelemetry.io/inject-nodejs: "true"`

## How should I manage dashboards?
Treat dashboards as code. Edit YAML under `charts/kof-dashboards/files/dashboards/*` and deploy via Helm/CI/CD. Avoid editing in the Grafana UI, as those changes will be overwritten.

## How do I avoid commingling tenant data?
Deploy collectors/storage per tenant namespace and restrict access with RBAC. This ensures separation of data paths per tenant.

## How do I control retention policies?
Configure the `retentionPeriod` for VictoriaMetrics and VictoriaLogs. You can find more information in [KOF Retention](./kof-retention.md).
--- a/kof-architecture.md
 b/kof-architecture.md
@@ -334,3 334,39 @@ 
 - [opentelemetry-kube-stack](https://github.com/open-telemetry/opentelemetry-helm-charts/tree/main/charts/opentelemetry-kube-stack) for hardware, OS, and Kubernetes metrics
 - [OpenCost](https://www.opencost.io/) "shines a light into the black box of Kubernetes spend"


## Deployment Scenarios

KOF supports two topologies:

### Production (Regional Clusters)
* Management-cluster telemetry is stored locally on the {{{ docsVersionInfo.k0rdentName }}} management cluster.
* Child workloads send telemetry to their regional cluster, supporting data sovereignty and isolation.

### Development / QA (Regionless)
* No regions are defined. All telemetry (management  child) is stored on the {{{ docsVersionInfo.k0rdentName }}} management cluster.

## Component Roles & Rationale

| Component | Role | Notes |
|---|---|---|
| k0rdent | Orchestration | Multi-cluster lifecycle  service templates |
| OpenTelemetry | Collection | Metrics, logs, traces; auto-instrumentation options |
| Promxy | Query Federation | Cross-cluster PromQL  alert rule evaluation at management |
| VictoriaMetrics | Metrics Storage | Scalable TSDB; selected over Prometheus for clustering  efficiency |
| VictoriaLogs | Log Storage | Scalable log TSDB with retention controls |
| Jaeger | Tracing | Trace store/visualization; regional awareness |
| Grafana | Visualization | Unified dashboards; SSO/RBAC |
| Dex | SSO | OIDC provider for Grafana |
| OpenCost | FinOps | Cost allocation and efficiency ratios |

## Dex Integration

KOF uses Dex as an identity provider to enable Single Signâ€‘On (SSO) with OAuth2 and OIDC.

- Authentication flow: Dex issues ID tokens to Grafana and other clients after authenticating against an upstream identity provider (IdP).
- External IdP integration: Dex can delegate to providers such as Okta, Entra ID, GitHub, or LDAP.
- Group membership mapping: Dex propagates group membership claims, which KOF uses to enforce RBAC. Grafana dashboards and KOF namespaces can be restricted based on these groups.

This model centralizes authentication, while authorization remains controlled via Kubernetes RBAC and Grafana roles.
--- a/kof-install.md
 b/kof-install.md
@@ -722,3 722,5 @@
     clusterctl describe cluster -n kcm-system $CHILD_CLUSTER_NAME \
       --show-conditions all
     ```

> ðŸ“˜ For guidance on long-term storage planning, review the [KOF Retention](./kof-retention.md) page, which covers retention and replication strategies.
--- a/kof-limits.md
 b/kof-limits.md
@@ -47,3 47,13 @@
     requests:
       memory: 128Mi
   ```

## Why VictoriaMetrics instead of Prometheus?

KOF uses VictoriaMetrics as the metrics backend, rather than Prometheus alone, for several reasons.

- Scalability: VictoriaMetrics supports horizontal clustering for large multi-cluster environments.
- Compression: VictoriaMetrics stores high-cardinality metrics efficiently.
- Federation: VictoriaMetrics integrates cleanly with Promxy for cross-cluster queries.

We still use Prometheus at the collection layer, but long-term storage and query efficiency are handled by VictoriaMetrics.
--- a/kof-storing.md
 b/kof-storing.md
@@ -344,3 344,5 @@
       "timestamp": 1744305535107,
       "message": "{\"body\":\"10.244.0.1 - - [10/Apr/2025 17:18:55] \\\"GET /-/ready HTTP/1.1 200 ...
     ```

> ðŸ“˜ See also: [KOF Retention](./kof-retention.md) for details on configuring retention periods and replication factors for VictoriaMetrics and VictoriaLogs.
--- a/kof-using.md
 b/kof-using.md
@@ -189,3 189,56 @@
 * SveltosCluster
 
 ![kof-ui-resources-monitoring](../../assets/kof/ui_resources_monitoring.gif)


## Dashboard Categories

KOF ships with dashboards across:
* Infrastructure: Provides infrastructure-related metrics, such as kube clusters, nodes, API server, networking, storage, or GPU.
* Applications: Provides metrics for applications, such as VictoriaMetrics, VictoriaLogs, Jaeger and OpenCost.
* Service Mesh: Provides metrics for service mesh, such as Istio control-plane and traffic.
* Platform: Provides metrics for the platform itself, including KCM, Cluster API, and Sveltos.

## Dashboard Lifecycle (GitOps Workflow)

All dashboards are managed as code to keep environments consistent. To add or change a dashboard, follow these steps:

**Add a new dashboard**
1. Create a YAML file under `charts/kof-dashboards/files/dashboards/` with the new dashboard definition.
2. Commit and push the change to Git.
3. Your CI/CD pipeline applies the Helm chart to the target cluster.

**Update an existing dashboard**
1. Edit the corresponding YAML file.
2. Commit and push changes.
3. CI/CD will roll out the update automatically.

**Delete a dashboard**
1. Remove the YAML file.
2. Commit and push changes.
3. CI/CD pipeline removes the dashboard from Grafana.

> WARNING: 
> Avoid editing dashboards directly in the Grafana UI. Changes will be overwritten by the next Helm release.

## Cost Management (OpenCost)

 KOF includes OpenCost, which provides cost management features for Kubernetes clusters. Common signals available in Grafana are:
* `node_total_hourly_cost` (per-node hourly cost)
* Namespace and pod-level cost allocation
* Historical spend trends and efficiency ratios

Once you have this information, you can optimize your cluster. Typical optimizations include:
* Identify under-utilized resources and right-size workloads
* Budgeting and monitoring with Grafana alerts

Common OpenCost metrics include:

| Metric | Description |
|--------|-------------|
| `node_total_hourly_cost` | Hourly cost per node (includes CPU, memory, storage) |
| `namespace_cpu_cost` | CPU cost aggregated by namespace |
| `namespace_memory_cost` | Memory cost aggregated by namespace |
| `pod_cost` | Cost allocation at pod granularity |
| `cluster_efficiency` | Ratio of requested vs actual resource usage |

These metrics appear in the pre-installed Grafana FinOps dashboards.

--- a/index.md
 b/index.md
@@ -57,6 57,26 @@
 Once you have KOF up and running,
 check [k0rdent/kof/docs](https://github.com/k0rdent/kof/tree/v{{{ extra.docsVersionInfo.kofVersions.kofDotVersion }}}/docs)
 for advanced guides.
 
-- [KOF Tracing](kof-tracing.md)
-- [KOF FAQ](kof-faq.md)

- [KOF Tracing](kof-tracing.md)
- [KOF FAQ](kof-faq.md)

## Managing KOF as Code

We recommend keeping a dedicated Git repository for your KOF deployment, separate from the {{{ docsVersionInfo.k0rdentName }}} repo itself and separate from application workload repos.

That repository should contain the Helm chart directories such as:

```
charts/
  kof-storage/
  kof-operators/
  kof-dashboards/
```

With this setup you can:
- Track all KOF configuration changes in Git.
- Use CI/CD pipelines to run `helm upgrade --install` against your management and regional clusters.
- Manage dashboards, retention policies, and observability settings as code for consistency across environments.

Other sections in this documentation, such as Dashboard Lifecycle and Retention assume you have such a repo and a CI/CD pipeline in place.
